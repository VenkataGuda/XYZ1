Professional Summary :  


1 year of professional IT experience which includes experience in Hadoop, Big data ecosystem related technologies.
Expertise in HDFS Architecture and Cluster concepts.
Experience on Apache Hadoop technologies,Hadoop distributed file system (HDFS), MapReduce framework,Pig, Hive, Sqoop and Flume and HBase.
Excellent understanding / knowledge of Hadoop architecture and various components such as HDFS, Job Tracker, Task Tracker, Name Node, Data Node, Resource Manager, Node Manager and Map Reduce.
Having good exposure in using Zookeeper.
Expertise in Hadoop Security with Kerberos.
Experience in managing and reviewing Hadoop log files.
Experience on Hadoop distributions like Cloudera.
Good knowledge in Linux as Hadoop runs on Linux.
Successfully engaged in extensive interaction with clients.
Provide 24X7 on-call supports and help desk.
Extremely motivated with good inter-personal Skills; have ability to work in strict deadlines.


Education Qualifications:
B.Tech  in Information Technology from JNTU University.

Experience:
Worked as S/W Engineer in HCL Technologies,Bangalore from Feb'14 to Apr'15.

Areas of Expertise: 


Big Data Ecosystems:	Map Reduce, HDFS, HBase, Hive, Pig, Sqoop, Flume.
Languages:	        SQL,PL/SQL, Unix shell Programming.
Databases:		Mysql, Hbase,PostgreSQL.
Tools:			Vmware,Ganglia,Cloudera Manager Enterprise.
Platforms:		Windows, Ubuntu, CentOS

Project Experience:																Feb'14  to Aprâ€™15
   There are two different claim systems that are maintained and administered and refered to as ECF(Electronic Claim file),Which is Legacy claim System and
   ATLAS(Accident Tracking Loss Assignment Scheduling),the new modren cliam system.There is a great potential for significant cost savings around adjuster 
   training,research and development,infrastructure provisioning and maintainance,operational support and external system integration.the Legacy Claim conversion
   will facilitate and retirement of primary claim system ECF,thereby eliminating the need to maintain two claim systems.This project also support the company's
   ultimate goal of eliminating  transactions and data from mainframe.

Responsibilities:

Hadoop - Installation, Configuration of clusters.
HDFS - monitoring, maintaining and upgrade.
Yarn - monitoring, troubleshooting, optimization.
Hadoop DataNodes Commissioning and Decommissioning.
Hadoop security setup with Kerberos.
Setup of Hadoop Administration with FS Image, Edit logs with Apache and Cloudera.
Worked with data delivery teams to setup new Hadoop users.
File system management and monitoring.
Involved in Backingup of namenodes metadata.
Involved in HDFS support and maintenance activities.
Installed MySQL Database on Linux.
HBASE/HIVE - setup, configuration. 
